"""
æ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯•ç³»ç»Ÿå„ç»„ä»¶çš„æ€§èƒ½è¡¨ç°
"""

import time
import asyncio
import threading
import statistics

from typing import List, Dict, Any

from datetime import datetime
import json

from pathlib import Path

# å¯¼å…¥æµ‹è¯•æ¨¡å—
try:
    from performance import (

        get_cache_manager, get_connection_pool, get_async_processor,
        get_performance_monitor
    )
    PERFORMANCE_AVAILABLE = True
except ImportError:
    PERFORMANCE_AVAILABLE = False

try:
    from security import get_input_validator, get_security_logger

    SECURITY_AVAILABLE = True
except ImportError:
    SECURITY_AVAILABLE = False


class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""


    def __init__(self):
        self.results = {}
        self.test_data = self._generate_test_data()


    def _generate_test_data(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        return {
            'small_text': "Hello, world!",
            'medium_text': "This is a medium-sized text for testing purposes. " * 10,
            'large_text': "This is a large text for performance testing. " * 100,
            'test_prompts': [
                "What is artificial intelligence?",
                "Explain machine learning",
                "How does deep learning work?",
                "What are neural networks?",
                "Describe natural language processing"
            ],
            'test_data_sizes': [100, 1000, 10000, 100000]
        }


    def run_all_benchmarks(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")

        if PERFORMANCE_AVAILABLE:
            self.results['cache_performance'] = self.benchmark_cache()
            self.results['connection_pool_performance'] = self.benchmark_connection_pool()
            self.results['async_processor_performance'] = self.benchmark_async_processor()
            self.results['performance_monitor_overhead'] = self.benchmark_performance_monitor()

        if SECURITY_AVAILABLE:
            self.results['security_validation_performance'] = self.benchmark_security_validation()

        self.results['system_performance'] = self.benchmark_system_performance()

        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report()

        return self.results


    def benchmark_cache(self) -> Dict[str, Any]:
        """ç¼“å­˜æ€§èƒ½æµ‹è¯•"""
        print("ğŸ“¦ æµ‹è¯•ç¼“å­˜æ€§èƒ½...")

        cache_manager = get_cache_manager()
        results = {}

        # å†™å…¥æ€§èƒ½æµ‹è¯•
        write_times = []
        for i in range(1000):
            start_time = time.time()
            cache_manager.set(f"test_key_{i}", f"test_value_{i}")
            write_times.append((time.time() - start_time) * 1000)

        results['write_performance'] = {
            'avg_ms': statistics.mean(write_times),
            'median_ms': statistics.median(write_times),
            'max_ms': max(write_times),
            'min_ms': min(write_times),
            'operations_per_second': 1000 / (sum(write_times) / 1000)
        }

        # è¯»å–æ€§èƒ½æµ‹è¯•
        read_times = []
        for i in range(1000):
            start_time = time.time()
            cache_manager.get(f"test_key_{i}")
            read_times.append((time.time() - start_time) * 1000)

        results['read_performance'] = {
            'avg_ms': statistics.mean(read_times),
            'median_ms': statistics.median(read_times),
            'max_ms': max(read_times),
            'min_ms': min(read_times),
            'operations_per_second': 1000 / (sum(read_times) / 1000)
        }

        # ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•
        hit_count = 0
        for i in range(1000):
            if cache_manager.get(f"test_key_{i}") is not None:
                hit_count += 1

        results['hit_rate'] = hit_count / 1000 * 100

        return results


    def benchmark_connection_pool(self) -> Dict[str, Any]:
        """è¿æ¥æ± æ€§èƒ½æµ‹è¯•"""
        print("ğŸ”— æµ‹è¯•è¿æ¥æ± æ€§èƒ½...")

        connection_pool = get_connection_pool()
        http_pool = connection_pool.get_http_pool()

        results = {}

        # è¿æ¥è·å–æ€§èƒ½æµ‹è¯•
        get_times = []
        sessions = []

        for i in range(100):
            start_time = time.time()
            session = http_pool.get_session()
            get_times.append((time.time() - start_time) * 1000)
            sessions.append(session)

        # å½’è¿˜è¿æ¥
        for session in sessions:
            http_pool.return_session(session)

        results['connection_acquisition'] = {
            'avg_ms': statistics.mean(get_times),
            'median_ms': statistics.median(get_times),
            'max_ms': max(get_times),
            'min_ms': min(get_times)
        }

        # å¹¶å‘è¿æ¥æµ‹è¯•


        def concurrent_connection_test():
            start_time = time.time()
            session = http_pool.get_session()
            time.sleep(0.1)  # æ¨¡æ‹Ÿä½¿ç”¨æ—¶é—´
            http_pool.return_session(session)
            return time.time() - start_time

        threads = []
        concurrent_times = []

        for i in range(20):
            thread = threading.Thread(target=lambda: concurrent_times.append(concurrent_connection_test()))
            threads.append(thread)
            thread.start()

        for thread in threads:
            thread.join()

        results['concurrent_performance'] = {
            'avg_ms': statistics.mean([t * 1000 for t in concurrent_times]),
            'max_ms': max([t * 1000 for t in concurrent_times]),
            'min_ms': min([t * 1000 for t in concurrent_times])
        }

        return results


    def benchmark_async_processor(self) -> Dict[str, Any]:
        """å¼‚æ­¥å¤„ç†å™¨æ€§èƒ½æµ‹è¯•"""
        print("âš¡ æµ‹è¯•å¼‚æ­¥å¤„ç†å™¨æ€§èƒ½...")

        async_processor = get_async_processor()
        results = {}

        # ä»»åŠ¡æäº¤æ€§èƒ½æµ‹è¯•


        def dummy_task(x):
            time.sleep(0.01)  # æ¨¡æ‹Ÿå·¥ä½œ
            return x * 2

        submit_times = []
        task_ids = []

        for i in range(100):
            start_time = time.time()
            task_id = async_processor.submit_task(dummy_task, i)
            submit_times.append((time.time() - start_time) * 1000)
            task_ids.append(task_id)

        results['task_submission'] = {
            'avg_ms': statistics.mean(submit_times),
            'median_ms': statistics.median(submit_times),
            'operations_per_second': 100 / (sum(submit_times) / 1000)
        }

        # ç­‰å¾…ä»»åŠ¡å®Œæˆå¹¶æµ‹è¯•å¤„ç†æ—¶é—´
        completion_times = []
        for task_id in task_ids:
            start_time = time.time()
            try:
                async_processor.wait_for_task(task_id, timeout=5)
                completion_times.append((time.time() - start_time) * 1000)
            except:
                pass

        if completion_times:
            results['task_completion'] = {
                'avg_ms': statistics.mean(completion_times),
                'median_ms': statistics.median(completion_times),
                'max_ms': max(completion_times),
                'min_ms': min(completion_times)
            }

        return results


    def benchmark_performance_monitor(self) -> Dict[str, Any]:
        """æ€§èƒ½ç›‘æ§å¼€é”€æµ‹è¯•"""
        print("ğŸ“Š æµ‹è¯•æ€§èƒ½ç›‘æ§å¼€é”€...")

        performance_monitor = get_performance_monitor()
        results = {}

        # æŒ‡æ ‡æ·»åŠ æ€§èƒ½æµ‹è¯•
        add_times = []
        for i in range(1000):
            start_time = time.time()
            performance_monitor.add_metric(f"test_metric_{i}", i, "count")
            add_times.append((time.time() - start_time) * 1000)

        results['metric_addition'] = {
            'avg_ms': statistics.mean(add_times),
            'median_ms': statistics.median(add_times),
            'operations_per_second': 1000 / (sum(add_times) / 1000)
        }

        # ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å¼€é”€
        collect_times = []
        for i in range(10):
            start_time = time.time()
            performance_monitor.get_current_system_status()
            collect_times.append((time.time() - start_time) * 1000)

        results['system_metrics_collection'] = {
            'avg_ms': statistics.mean(collect_times),
            'max_ms': max(collect_times),
            'min_ms': min(collect_times)
        }

        return results


    def benchmark_security_validation(self) -> Dict[str, Any]:
        """å®‰å…¨éªŒè¯æ€§èƒ½æµ‹è¯•"""
        print("ğŸ›¡ï¸ æµ‹è¯•å®‰å…¨éªŒè¯æ€§èƒ½...")

        input_validator = get_input_validator()
        results = {}

        # æ–‡æœ¬éªŒè¯æ€§èƒ½æµ‹è¯•
        validation_times = []
        test_texts = [
            self.test_data['small_text'],
            self.test_data['medium_text'],
            self.test_data['large_text']
        ]

        for text in test_texts:
            for i in range(100):
                start_time = time.time()
                input_validator.validate_text_input(text)
                validation_times.append((time.time() - start_time) * 1000)

        results['text_validation'] = {
            'avg_ms': statistics.mean(validation_times),
            'median_ms': statistics.median(validation_times),
            'operations_per_second': len(validation_times) / (sum(validation_times) / 1000)
        }

        return results


    def benchmark_system_performance(self) -> Dict[str, Any]:
        """ç³»ç»Ÿæ•´ä½“æ€§èƒ½æµ‹è¯•"""
        print("ğŸ–¥ï¸ æµ‹è¯•ç³»ç»Ÿæ•´ä½“æ€§èƒ½...")

        results = {}

        # CPUå¯†é›†å‹ä»»åŠ¡æµ‹è¯•


        def cpu_intensive_task():
            start_time = time.time()
            # è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—


            def fibonacci(n):
                if n <= 1:
                    return n
                return fibonacci(n-1) + fibonacci(n-2)

            fibonacci(25)
            return time.time() - start_time

        cpu_times = []
        for i in range(10):
            cpu_times.append(cpu_intensive_task() * 1000)

        results['cpu_intensive'] = {
            'avg_ms': statistics.mean(cpu_times),
            'median_ms': statistics.median(cpu_times),
            'max_ms': max(cpu_times),
            'min_ms': min(cpu_times)
        }

        # å†…å­˜ä½¿ç”¨æµ‹è¯•
        import psutil

        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB

        # åˆ›å»ºå¤§é‡æ•°æ®
        large_data = []
        for i in range(10000):
            large_data.append(f"test_data_{i}" * 100)

        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_used = memory_after - memory_before

        results['memory_usage'] = {
            'before_mb': memory_before,
            'after_mb': memory_after,
            'used_mb': memory_used
        }

        # æ¸…ç†å†…å­˜
        del large_data

        return results


    def _generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'test_results': self.results,
            'summary': self._generate_summary()
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        report_file = Path('logs') / f'benchmark_report_{int(time.time())}.json'
        report_file.parent.mkdir(exist_ok=True)

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        # æ‰“å°æ‘˜è¦
        self._print_summary()


    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        summary = {}

        if 'cache_performance' in self.results:
            cache_results = self.results['cache_performance']
            summary['cache'] = {
                'read_ops_per_sec': cache_results['read_performance']['operations_per_second'],
                'write_ops_per_sec': cache_results['write_performance']['operations_per_second'],
                'hit_rate': cache_results['hit_rate']
            }

        if 'async_processor_performance' in self.results:
            async_results = self.results['async_processor_performance']
            summary['async_processor'] = {
                'submission_ops_per_sec': async_results['task_submission']['operations_per_second']
            }

        return summary


    def _print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("="*60)

        if 'cache_performance' in self.results:
            cache = self.results['cache_performance']
            print(f"\nğŸ“¦ ç¼“å­˜æ€§èƒ½:")
            print(f"  è¯»å–: {cache['read_performance']['operations_per_second']:.0f} ops/sec")
            print(f"  å†™å…¥: {cache['write_performance']['operations_per_second']:.0f} ops/sec")
            print(f"  å‘½ä¸­ç‡: {cache['hit_rate']:.1f}%")

        if 'connection_pool_performance' in self.results:
            pool = self.results['connection_pool_performance']
            print(f"\nğŸ”— è¿æ¥æ± æ€§èƒ½:")
            print(f"  è¿æ¥è·å–: {pool['connection_acquisition']['avg_ms']:.2f}ms (å¹³å‡)")
            print(f"  å¹¶å‘å¤„ç†: {pool['concurrent_performance']['avg_ms']:.2f}ms (å¹³å‡)")

        if 'async_processor_performance' in self.results:
            async_proc = self.results['async_processor_performance']
            print(f"\nâš¡ å¼‚æ­¥å¤„ç†å™¨æ€§èƒ½:")
            print(f"  ä»»åŠ¡æäº¤: {async_proc['task_submission']['operations_per_second']:.0f} ops/sec")
            if 'task_completion' in async_proc:
                print(f"  ä»»åŠ¡å®Œæˆ: {async_proc['task_completion']['avg_ms']:.2f}ms (å¹³å‡)")

        if 'security_validation_performance' in self.results:
            security = self.results['security_validation_performance']
            print(f"\nğŸ›¡ï¸ å®‰å…¨éªŒè¯æ€§èƒ½:")
            print(f"  æ–‡æœ¬éªŒè¯: {security['text_validation']['operations_per_second']:.0f} ops/sec")

        if 'system_performance' in self.results:
            system = self.results['system_performance']
            print(f"\nğŸ–¥ï¸ ç³»ç»Ÿæ€§èƒ½:")
            print(f"  CPUå¯†é›†å‹ä»»åŠ¡: {system['cpu_intensive']['avg_ms']:.2f}ms (å¹³å‡)")
            print(f"  å†…å­˜ä½¿ç”¨: {system['memory_usage']['used_mb']:.2f}MB")

        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    benchmark = BenchmarkRunner()
    benchmark.run_all_benchmarks()

if __name__ == "__main__":
    main()
