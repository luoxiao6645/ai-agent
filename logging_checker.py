#!/usr/bin/env python3
"""
æ—¥å¿—è®°å½•è§„èŒƒæ€§æ£€æŸ¥å·¥å…·
éªŒè¯æ—¥å¿—çº§åˆ«ã€æ ¼å¼å’Œå®‰å…¨æ€§
"""

import re
import ast

from pathlib import Path

from typing import Dict, List, Any

from datetime import datetime


class LoggingChecker:
    """æ—¥å¿—è®°å½•æ£€æŸ¥å™¨"""


    def __init__(self):
        self.project_root = Path(".")
        self.python_files = []
        self.results = {}

        # æ—¥å¿—çº§åˆ«
        self.log_levels = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']

        # æ•æ„Ÿä¿¡æ¯æ¨¡å¼
        self.sensitive_patterns = [
            r'password\s*=\s*["\'].*["\']',
            r'token\s*=\s*["\'].*["\']',
            r'key\s*=\s*["\'].*["\']',
            r'secret\s*=\s*["\'].*["\']',
            r'api_key\s*=\s*["\'].*["\']',
            r'access_token\s*=\s*["\'].*["\']'
        ]


    def scan_files(self):
        """æ‰«æPythonæ–‡ä»¶"""
        print("ğŸ” æ‰«æPythonæ–‡ä»¶...")

        exclude_patterns = ['__pycache__', '.git', 'venv', 'env']

        for file_path in self.project_root.rglob("*.py"):
            if not any(pattern in str(file_path) for pattern in exclude_patterns):
                self.python_files.append(file_path)

        print(f"ğŸ“ æ‰¾åˆ° {len(self.python_files)} ä¸ªPythonæ–‡ä»¶")


    def analyze_logging_usage(self):
        """åˆ†ææ—¥å¿—ä½¿ç”¨æƒ…å†µ"""
        print("\nğŸ“ åˆ†ææ—¥å¿—ä½¿ç”¨æƒ…å†µ...")

        logging_analysis = {
            "files_with_logging": 0,
            "total_log_statements": 0,
            "log_level_distribution": {level: 0 for level in self.log_levels},
            "logging_imports": 0,
            "print_statements": 0,
            "files_using_print": 0,
            "logger_configurations": 0,
            "log_statements_by_file": {}
        }

        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.split('\n')

                file_has_logging = False
                file_has_print = False
                file_log_count = 0
                file_print_count = 0

                for i, line in enumerate(lines):
                    stripped_line = line.strip()

                    # æ£€æŸ¥loggingå¯¼å…¥
                    if re.search(r'import\s+logging|from\s+logging', stripped_line):
                        logging_analysis["logging_imports"] += 1

                    # æ£€æŸ¥loggeré…ç½®
                    if 'logging.basicConfig' in stripped_line or 'getLogger' in stripped_line:
                        logging_analysis["logger_configurations"] += 1

                    # æ£€æŸ¥æ—¥å¿—è¯­å¥
                    for level in self.log_levels:
                        if f'logger.{level.lower()}' in stripped_line or f'logging.{level.lower()}' in stripped_line:
                            logging_analysis["total_log_statements"] += 1
                            logging_analysis["log_level_distribution"][level] += 1
                            file_has_logging = True
                            file_log_count += 1

                    # æ£€æŸ¥printè¯­å¥
                    if re.search(r'\bprint\s*\(', stripped_line) and not stripped_line.startswith('#'):
                        logging_analysis["print_statements"] += 1
                        file_has_print = True
                        file_print_count += 1

                if file_has_logging:
                    logging_analysis["files_with_logging"] += 1

                if file_has_print:
                    logging_analysis["files_using_print"] += 1

                logging_analysis["log_statements_by_file"][str(file_path)] = {
                    "log_statements": file_log_count,
                    "print_statements": file_print_count
                }

            except Exception as e:
                print(f"âš ï¸ åˆ†æå¤±è´¥ {file_path}: {e}")

        self.results["logging_usage"] = logging_analysis

        print(f"ğŸ“Š æ—¥å¿—ä½¿ç”¨åˆ†æ:")
        print(f"  - ä½¿ç”¨æ—¥å¿—çš„æ–‡ä»¶: {logging_analysis['files_with_logging']}")
        print(f"  - æ—¥å¿—è¯­å¥æ€»æ•°: {logging_analysis['total_log_statements']}")
        print(f"  - ä½¿ç”¨printçš„æ–‡ä»¶: {logging_analysis['files_using_print']}")
        print(f"  - printè¯­å¥æ€»æ•°: {logging_analysis['print_statements']}")


    def check_log_level_appropriateness(self):
        """æ£€æŸ¥æ—¥å¿—çº§åˆ«çš„åˆç†æ€§"""
        print("\nğŸ“Š æ£€æŸ¥æ—¥å¿—çº§åˆ«åˆç†æ€§...")

        level_analysis = {
            "appropriate_usage": [],
            "questionable_usage": [],
            "missing_error_logs": [],
            "excessive_debug_logs": []
        }

        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.split('\n')

                debug_count = 0

                for i, line in enumerate(lines):
                    stripped_line = line.strip()

                    # æ£€æŸ¥DEBUGçº§åˆ«ä½¿ç”¨
                    if 'logger.debug' in stripped_line or 'logging.debug' in stripped_line:
                        debug_count += 1
                        if debug_count > 10:  # è¿‡å¤šçš„debugæ—¥å¿—
                            level_analysis["excessive_debug_logs"].append({
                                "file": str(file_path),
                                "line": i + 1,
                                "issue": "æ–‡ä»¶ä¸­DEBUGæ—¥å¿—è¿‡å¤š"
                            })

                    # æ£€æŸ¥å¼‚å¸¸å¤„ç†ä¸­æ˜¯å¦æœ‰é”™è¯¯æ—¥å¿—
                    if stripped_line.startswith('except'):
                        # æŸ¥çœ‹æ¥ä¸‹æ¥å‡ è¡Œæ˜¯å¦æœ‰é”™è¯¯æ—¥å¿—
                        has_error_log = False
                        for j in range(i + 1, min(i + 5, len(lines))):
                            next_line = lines[j].strip()
                            if ('logger.error' in next_line or 'logger.exception' in next_line or
                                'logging.error' in next_line or 'logging.exception' in next_line):
                                has_error_log = True
                                level_analysis["appropriate_usage"].append({
                                    "file": str(file_path),
                                    "line": j + 1,
                                    "practice": "åœ¨å¼‚å¸¸å¤„ç†ä¸­ä½¿ç”¨äº†é”™è¯¯æ—¥å¿—"
                                })
                                break

                        if not has_error_log:
                            level_analysis["missing_error_logs"].append({
                                "file": str(file_path),
                                "line": i + 1,
                                "issue": "å¼‚å¸¸å¤„ç†ä¸­ç¼ºå°‘é”™è¯¯æ—¥å¿—"
                            })

                    # æ£€æŸ¥INFOçº§åˆ«çš„åˆç†ä½¿ç”¨
                    if 'logger.info' in stripped_line or 'logging.info' in stripped_line:
                        # æ£€æŸ¥æ˜¯å¦åœ¨é‡è¦æ“ä½œä¸­ä½¿ç”¨
                        if any(keyword in stripped_line.lower() for keyword in
                              ['started', 'completed', 'initialized', 'success', 'æˆåŠŸ', 'å¼€å§‹', 'å®Œæˆ']):
                            level_analysis["appropriate_usage"].append({
                                "file": str(file_path),
                                "line": i + 1,
                                "practice": "åœ¨é‡è¦æ“ä½œä¸­ä½¿ç”¨äº†INFOæ—¥å¿—"
                            })

            except Exception as e:
                print(f"âš ï¸ æ—¥å¿—çº§åˆ«æ£€æŸ¥å¤±è´¥ {file_path}: {e}")

        self.results["log_level_analysis"] = level_analysis

        print(f"ğŸ“Š æ—¥å¿—çº§åˆ«åˆ†æ:")
        print(f"  - åˆç†ä½¿ç”¨: {len(level_analysis['appropriate_usage'])}")
        print(f"  - å¯ç–‘ä½¿ç”¨: {len(level_analysis['questionable_usage'])}")
        print(f"  - ç¼ºå°‘é”™è¯¯æ—¥å¿—: {len(level_analysis['missing_error_logs'])}")
        print(f"  - DEBUGæ—¥å¿—è¿‡å¤š: {len(level_analysis['excessive_debug_logs'])}")


    def check_sensitive_information(self):
        """æ£€æŸ¥æ•æ„Ÿä¿¡æ¯æ³„éœ²"""
        print("\nğŸ”’ æ£€æŸ¥æ•æ„Ÿä¿¡æ¯æ³„éœ²...")

        security_analysis = {
            "potential_leaks": [],
            "safe_logging": [],
            "files_with_risks": 0
        }

        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.split('\n')

                file_has_risks = False

                for i, line in enumerate(lines):
                    stripped_line = line.strip()

                    # æ£€æŸ¥æ—¥å¿—è¯­å¥ä¸­çš„æ•æ„Ÿä¿¡æ¯
                    if any(log_pattern in stripped_line for log_pattern in
                          ['logger.', 'logging.', 'print(']):

                        # æ£€æŸ¥æ•æ„Ÿä¿¡æ¯æ¨¡å¼
                        for pattern in self.sensitive_patterns:
                            if re.search(pattern, stripped_line, re.IGNORECASE):
                                security_analysis["potential_leaks"].append({
                                    "file": str(file_path),
                                    "line": i + 1,
                                    "code": stripped_line,
                                    "risk": "å¯èƒ½æ³„éœ²æ•æ„Ÿä¿¡æ¯"
                                })
                                file_has_risks = True

                        # æ£€æŸ¥æ˜¯å¦æœ‰å®‰å…¨çš„æ—¥å¿—è®°å½•å®è·µ
                        if any(safe_pattern in stripped_line.lower() for safe_pattern in
                              ['***', 'masked', 'hidden', 'éšè—', 'æ©ç ']):
                            security_analysis["safe_logging"].append({
                                "file": str(file_path),
                                "line": i + 1,
                                "practice": "ä½¿ç”¨äº†å®‰å…¨çš„æ—¥å¿—è®°å½•æ–¹å¼"
                            })

                if file_has_risks:
                    security_analysis["files_with_risks"] += 1

            except Exception as e:
                print(f"âš ï¸ å®‰å…¨æ£€æŸ¥å¤±è´¥ {file_path}: {e}")

        self.results["security_analysis"] = security_analysis

        print(f"ğŸ“Š å®‰å…¨åˆ†æ:")
        print(f"  - æ½œåœ¨æ³„éœ²é£é™©: {len(security_analysis['potential_leaks'])}")
        print(f"  - å®‰å…¨æ—¥å¿—å®è·µ: {len(security_analysis['safe_logging'])}")
        print(f"  - æœ‰é£é™©çš„æ–‡ä»¶: {security_analysis['files_with_risks']}")


    def check_log_format_consistency(self):
        """æ£€æŸ¥æ—¥å¿—æ ¼å¼ä¸€è‡´æ€§"""
        print("\nğŸ“ æ£€æŸ¥æ—¥å¿—æ ¼å¼ä¸€è‡´æ€§...")

        format_analysis = {
            "consistent_formats": 0,
            "inconsistent_formats": 0,
            "format_issues": [],
            "good_practices": []
        }

        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.split('\n')

                log_formats = []

                for i, line in enumerate(lines):
                    stripped_line = line.strip()

                    # æŸ¥æ‰¾æ—¥å¿—æ ¼å¼é…ç½®
                    if 'format=' in stripped_line and 'logging' in stripped_line:
                        log_formats.append({
                            "file": str(file_path),
                            "line": i + 1,
                            "format": stripped_line
                        })

                    # æ£€æŸ¥æ—¥å¿—æ¶ˆæ¯æ ¼å¼
                    if any(log_pattern in stripped_line for log_pattern in
                          ['logger.', 'logging.']):

                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
                        if any(context in stripped_line for context in
                              ['%s', '{', 'f"', "f'"]):
                            format_analysis["good_practices"].append({
                                "file": str(file_path),
                                "line": i + 1,
                                "practice": "æ—¥å¿—æ¶ˆæ¯åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯"
                            })

                        # æ£€æŸ¥ç¡¬ç¼–ç çš„æ—¥å¿—æ¶ˆæ¯
                        if re.search(r'logger\.\w+\s*\(\s*["\'][^"\']*["\']\s*\)', stripped_line):
                            format_analysis["format_issues"].append({
                                "file": str(file_path),
                                "line": i + 1,
                                "issue": "ä½¿ç”¨äº†ç¡¬ç¼–ç çš„æ—¥å¿—æ¶ˆæ¯",
                                "suggestion": "è€ƒè™‘æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯"
                            })

            except Exception as e:
                print(f"âš ï¸ æ ¼å¼æ£€æŸ¥å¤±è´¥ {file_path}: {e}")

        self.results["format_analysis"] = format_analysis

        print(f"ğŸ“Š æ ¼å¼åˆ†æ:")
        print(f"  - è‰¯å¥½å®è·µ: {len(format_analysis['good_practices'])}")
        print(f"  - æ ¼å¼é—®é¢˜: {len(format_analysis['format_issues'])}")


    def generate_report(self):
        """ç”Ÿæˆæ—¥å¿—è®°å½•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“ æ—¥å¿—è®°å½•è§„èŒƒæ€§åˆ†ææŠ¥å‘Š")
        print("="*60)

        logging_usage = self.results["logging_usage"]
        total_files = len(self.python_files)

        # è®¡ç®—æ—¥å¿—è¦†ç›–ç‡
        logging_coverage = (logging_usage["files_with_logging"] / total_files) * 100

        print(f"ğŸ“Š æ—¥å¿—è¦†ç›–ç‡: {logging_coverage:.1f}%")
        print(f"ğŸ“ åˆ†ææ–‡ä»¶æ€»æ•°: {total_files}")
        print(f"ğŸ“ ä½¿ç”¨æ—¥å¿—çš„æ–‡ä»¶: {logging_usage['files_with_logging']}")
        print(f"ğŸ–¨ï¸ ä½¿ç”¨printçš„æ–‡ä»¶: {logging_usage['files_using_print']}")

        # æ—¥å¿—çº§åˆ«åˆ†å¸ƒ
        print(f"\nğŸ“Š æ—¥å¿—çº§åˆ«åˆ†å¸ƒ:")
        for level, count in logging_usage["log_level_distribution"].items():
            print(f"  - {level}: {count}")

        # é—®é¢˜ç»Ÿè®¡
        total_issues = (
            len(self.results["log_level_analysis"]["missing_error_logs"]) +
            len(self.results["log_level_analysis"]["excessive_debug_logs"]) +
            len(self.results["security_analysis"]["potential_leaks"]) +
            len(self.results["format_analysis"]["format_issues"])
        )

        print(f"\nâš ï¸ å‘ç°çš„é—®é¢˜:")
        print(f"  - ç¼ºå°‘é”™è¯¯æ—¥å¿—: {len(self.results['log_level_analysis']['missing_error_logs'])}")
        print(f"  - DEBUGæ—¥å¿—è¿‡å¤š: {len(self.results['log_level_analysis']['excessive_debug_logs'])}")
        print(f"  - æ½œåœ¨ä¿¡æ¯æ³„éœ²: {len(self.results['security_analysis']['potential_leaks'])}")
        print(f"  - æ ¼å¼é—®é¢˜: {len(self.results['format_analysis']['format_issues'])}")
        print(f"  - æ€»é—®é¢˜æ•°: {total_issues}")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        self.results["summary"] = {
            "timestamp": datetime.now().isoformat(),
            "total_files": total_files,
            "logging_coverage": logging_coverage,
            "total_issues": total_issues
        }

        import json

        with open("logging_analysis_report.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: logging_analysis_report.json")

        return self.results


    def run_analysis(self):
        """è¿è¡Œå®Œæ•´çš„æ—¥å¿—åˆ†æ"""
        self.scan_files()
        self.analyze_logging_usage()
        self.check_log_level_appropriateness()
        self.check_sensitive_information()
        self.check_log_format_consistency()
        return self.generate_report()


def main():
    checker = LoggingChecker()
    return checker.run_analysis()

if __name__ == "__main__":
    main()
