"""
ä¼˜åŒ–ç‰ˆæ™ºèƒ½å¤šæ¨¡æ€AI Agent - Streamlitåº”ç”¨
ç¬¬äºŒé˜¶æ®µï¼šæ€§èƒ½å’Œç”¨æˆ·ä½“éªŒä¼˜åŒ–
"""
import streamlit as st
import os
import sys
import asyncio
import json
import time
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

# å¯¼å…¥ä¼˜åŒ–å·¥å…·
try:
    from utils.cache_manager import get_cache_manager
    from utils.async_manager import get_async_manager, get_progress_tracker
    from utils.memory_optimizer import get_memory_optimizer
    from utils.ui_components import get_ui_components
    OPTIMIZATION_AVAILABLE = True
except ImportError:
    OPTIMIZATION_AVAILABLE = False

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é¡µé¢é…ç½®å·²åœ¨app.pyä¸­è®¾ç½®ï¼Œè¿™é‡Œä¸å†é‡å¤è®¾ç½®

# åˆå§‹åŒ–ä¼˜åŒ–ç»„ä»¶
if OPTIMIZATION_AVAILABLE:
    cache_manager = get_cache_manager()
    async_manager = get_async_manager()
    progress_tracker = get_progress_tracker()
    memory_optimizer = get_memory_optimizer()
    ui_components = get_ui_components()

# è‡ªå®šä¹‰CSSæ ·å¼
def load_custom_css():
    """åŠ è½½è‡ªå®šä¹‰CSSæ ·å¼"""
    st.markdown("""
    <style>
    /* ä¸»é¢˜è‰²å½© */
    :root {
        --primary-color: #FF6B6B;
        --secondary-color: #4ECDC4;
        --accent-color: #45B7D1;
        --background-color: #F8F9FA;
        --text-color: #2C3E50;
    }
    
    /* ä¸»å®¹å™¨æ ·å¼ */
    .main-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 10px 30px rgba(0,0,0,0.1);
    }
    
    /* å¡ç‰‡æ ·å¼ */
    .feature-card {
        background: white;
        padding: 1.5rem;
        border-radius: 12px;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        margin: 1rem 0;
        border-left: 4px solid var(--primary-color);
        transition: transform 0.3s ease;
    }
    
    .feature-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 8px 25px rgba(0,0,0,0.15);
    }
    
    /* çŠ¶æ€æŒ‡ç¤ºå™¨ */
    .status-indicator {
        display: inline-block;
        width: 12px;
        height: 12px;
        border-radius: 50%;
        margin-right: 8px;
    }
    
    .status-online { background-color: #2ECC71; }
    .status-warning { background-color: #F39C12; }
    .status-error { background-color: #E74C3C; }
    
    /* æ€§èƒ½æŒ‡æ ‡æ ·å¼ */
    .metric-container {
        background: linear-gradient(45deg, #f093fb 0%, #f5576c 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin: 0.5rem 0;
    }
    
    /* è¿›åº¦æ¡æ ·å¼ */
    .stProgress > div > div > div > div {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    }
    
    /* æŒ‰é’®æ ·å¼ */
    .stButton > button {
        background: linear-gradient(45deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.5rem 1rem;
        font-weight: 600;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    }
    
    /* ä¾§è¾¹æ æ ·å¼ */
    .css-1d391kg {
        background: linear-gradient(180deg, #667eea 0%, #764ba2 100%);
    }
    
    /* å“åº”å¼è®¾è®¡ */
    @media (max-width: 768px) {
        .main-container {
            padding: 1rem;
            margin: 0.5rem 0;
        }
        
        .feature-card {
            padding: 1rem;
            margin: 0.5rem 0;
        }
    }
    
    /* åŠ¨ç”»æ•ˆæœ */
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(20px); }
        to { opacity: 1; transform: translateY(0); }
    }
    
    .fade-in {
        animation: fadeIn 0.6s ease-out;
    }
    
    /* åŠ è½½åŠ¨ç”» */
    .loading-spinner {
        border: 4px solid #f3f3f3;
        border-top: 4px solid var(--primary-color);
        border-radius: 50%;
        width: 40px;
        height: 40px;
        animation: spin 1s linear infinite;
        margin: 20px auto;
    }
    
    @keyframes spin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
    }
    </style>
    """, unsafe_allow_html=True)

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ˆå¸¦ç¼“å­˜ï¼‰
@st.cache_resource
def init_client():
    """åˆå§‹åŒ–AIå®¢æˆ·ç«¯"""
    try:
        client = OpenAI(
            base_url=os.getenv("ARK_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3"),
            api_key=os.getenv("ARK_API_KEY"),
        )
        return client
    except Exception as e:
        st.error(f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

# å°è¯•å¯¼å…¥å®Œæ•´ç³»ç»Ÿç»„ä»¶
def try_import_full_system():
    """å°è¯•å¯¼å…¥å®Œæ•´ç³»ç»Ÿç»„ä»¶"""
    try:
        # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        
        from multimodal_agent.core.agent import MultiModalAgent
        from multimodal_agent.tools.tool_manager import ToolManager
        from config import Config
        return True, MultiModalAgent, ToolManager, Config
    except Exception as e:
        return False, None, None, None

# æ£€æŸ¥ç³»ç»Ÿèƒ½åŠ›
FULL_SYSTEM_AVAILABLE, MultiModalAgent, ToolManager, Config = try_import_full_system()

def main():
    """ä¸»ç•Œé¢"""
    # åŠ è½½è‡ªå®šä¹‰æ ·å¼
    load_custom_css()
    
    # å†…å­˜ç›‘æ§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if OPTIMIZATION_AVAILABLE:
        memory_optimizer.monitor_memory_usage()
    
    # ä¸»æ ‡é¢˜
    st.markdown("""
    <div class="main-container fade-in">
        <h1 style="text-align: center; color: white; margin-bottom: 0;">
            ğŸ¤– æ™ºèƒ½å¤šæ¨¡æ€AI Agentç³»ç»Ÿ
        </h1>
        <p style="text-align: center; color: rgba(255,255,255,0.8); font-size: 1.2em;">
            ç¬¬äºŒé˜¶æ®µä¼˜åŒ–ç‰ˆ - æ€§èƒ½ä¸ä½“éªŒåŒé‡æå‡
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # ç³»ç»ŸçŠ¶æ€æ˜¾ç¤º
    display_system_status()
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = init_client()
    if not client:
        st.error("âŒ AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®")
        return
    
    # ä¾§è¾¹æ 
    render_enhanced_sidebar(client)
    
    # ä¸»ç•Œé¢æ ‡ç­¾é¡µ
    if FULL_SYSTEM_AVAILABLE and OPTIMIZATION_AVAILABLE:
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ’¬ æ™ºèƒ½å¯¹è¯", "ğŸ“ æ–‡ä»¶å¤„ç†", "ğŸ§® å·¥å…·ç®±", "ğŸ§  è®°å¿†ç®¡ç†", "ğŸ“Š æ€§èƒ½ç›‘æ§"
        ])
        
        with tab1:
            enhanced_chat_interface(client)
        
        with tab2:
            enhanced_file_interface(client)
        
        with tab3:
            enhanced_tools_interface(client)
        
        with tab4:
            memory_interface()
        
        with tab5:
            performance_monitoring_interface()
    else:
        tab1, tab2, tab3 = st.tabs(["ğŸ’¬ æ™ºèƒ½å¯¹è¯", "ğŸ“ æ–‡ä»¶å¤„ç†", "ğŸ§® å·¥å…·ç®±"])
        
        with tab1:
            optimized_chat_interface(client)
        
        with tab2:
            optimized_file_interface(client)
        
        with tab3:
            optimized_tools_interface(client)

def display_system_status():
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if FULL_SYSTEM_AVAILABLE:
            st.markdown("""
            <div class="feature-card">
                <div class="status-indicator status-online"></div>
                <strong>å®Œæ•´ç³»ç»Ÿ</strong><br>
                <small>å¤šæ¨¡æ€Agentå·²å¯ç”¨</small>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown("""
            <div class="feature-card">
                <div class="status-indicator status-warning"></div>
                <strong>ç®€åŒ–æ¨¡å¼</strong><br>
                <small>åŸºç¡€åŠŸèƒ½å¯ç”¨</small>
            </div>
            """, unsafe_allow_html=True)
    
    with col2:
        if OPTIMIZATION_AVAILABLE:
            st.markdown("""
            <div class="feature-card">
                <div class="status-indicator status-online"></div>
                <strong>æ€§èƒ½ä¼˜åŒ–</strong><br>
                <small>ç¼“å­˜å’Œå¼‚æ­¥å·²å¯ç”¨</small>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown("""
            <div class="feature-card">
                <div class="status-indicator status-warning"></div>
                <strong>æ ‡å‡†æ¨¡å¼</strong><br>
                <small>åŸºç¡€æ€§èƒ½</small>
            </div>
            """, unsafe_allow_html=True)
    
    with col3:
        current_time = datetime.now().strftime("%H:%M:%S")
        st.markdown(f"""
        <div class="feature-card">
            <div class="status-indicator status-online"></div>
            <strong>ç³»ç»Ÿæ—¶é—´</strong><br>
            <small>{current_time}</small>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        if OPTIMIZATION_AVAILABLE:
            memory_usage = memory_optimizer.get_memory_usage()
            memory_mb = memory_usage.get('rss_mb', 0)
            st.markdown(f"""
            <div class="feature-card">
                <div class="status-indicator status-online"></div>
                <strong>å†…å­˜ä½¿ç”¨</strong><br>
                <small>{memory_mb:.1f} MB</small>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown("""
            <div class="feature-card">
                <div class="status-indicator status-warning"></div>
                <strong>å†…å­˜ç›‘æ§</strong><br>
                <small>ä¸å¯ç”¨</small>
            </div>
            """, unsafe_allow_html=True)

def render_enhanced_sidebar(client):
    """æ¸²æŸ“å¢å¼ºä¾§è¾¹æ """
    with st.sidebar:
        st.markdown("## ğŸ› ï¸ ç³»ç»Ÿæ§åˆ¶")
        
        # ç³»ç»Ÿä¿¡æ¯
        with st.expander("ğŸ“Š ç³»ç»Ÿä¿¡æ¯", expanded=True):
            st.success("âœ… AIå®¢æˆ·ç«¯å·²è¿æ¥")
            st.info(f"ğŸ•’ {datetime.now().strftime('%H:%M:%S')}")
            
            if OPTIMIZATION_AVAILABLE:
                # ç¼“å­˜ç»Ÿè®¡
                cache_stats = cache_manager.get_cache_stats()
                st.metric("ç¼“å­˜å‘½ä¸­ç‡", f"{cache_stats['hit_rate']:.1f}%")
                st.metric("ç¼“å­˜å¤§å°", f"{cache_stats['cache_size']}")
        
        # æ¨¡å‹é…ç½®
        with st.expander("âš™ï¸ æ¨¡å‹é…ç½®"):
            model = st.selectbox(
                "é€‰æ‹©æ¨¡å‹",
                [os.getenv("ARK_MODEL", "ep-20250506230532-w7rdw")],
                index=0
            )
            
            temperature = st.slider("æ¸©åº¦", 0.0, 1.0, 0.7, 0.1)
            max_tokens = st.slider("æœ€å¤§ä»¤ç‰Œæ•°", 100, 2000, 500, 100)
        
        # ç³»ç»Ÿæ“ä½œ
        with st.expander("ğŸ”§ ç³»ç»Ÿæ“ä½œ"):
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯"):
                    st.session_state.messages = []
                    st.session_state.conversation_history = []
                    st.success("å¯¹è¯å·²æ¸…é™¤")
            
            with col2:
                if st.button("ğŸ§¹ æ¸…ç†ç¼“å­˜") and OPTIMIZATION_AVAILABLE:
                    cache_manager.clear_cache()
                    st.success("ç¼“å­˜å·²æ¸…ç†")
            
            if OPTIMIZATION_AVAILABLE:
                if st.button("ğŸ”„ å†…å­˜ä¼˜åŒ–"):
                    result = memory_optimizer.auto_cleanup()
                    st.success(f"å·²æ¸…ç† {result.get('session_cleaned', 0)} é¡¹")
        
        # æ€§èƒ½ç›‘æ§
        if OPTIMIZATION_AVAILABLE:
            with st.expander("ğŸ“ˆ æ€§èƒ½ç›‘æ§"):
                memory_stats = memory_optimizer.get_optimization_stats()
                
                st.metric("å½“å‰å†…å­˜", f"{memory_stats['current_usage']['rss_mb']:.1f} MB")
                st.metric("å³°å€¼å†…å­˜", f"{memory_stats['peak_usage']:.1f} MB")
                st.metric("æ¸…ç†æ¬¡æ•°", memory_stats['cleanup_count'])
                
                # å†…å­˜è¶‹åŠ¿
                trend_emoji = {
                    'increasing': 'ğŸ“ˆ',
                    'decreasing': 'ğŸ“‰', 
                    'stable': 'â¡ï¸',
                    'unknown': 'â“'
                }
                st.info(f"å†…å­˜è¶‹åŠ¿: {trend_emoji.get(memory_stats['trend'], 'â“')} {memory_stats['trend']}")

def enhanced_chat_interface(client):
    """å¢å¼ºç‰ˆå¯¹è¯ç•Œé¢ï¼ˆå®Œæ•´ç³»ç»Ÿ+ä¼˜åŒ–ï¼‰"""
    st.markdown("""
    <div class="feature-card fade-in">
        <h3>ğŸ’¬ æ™ºèƒ½å¯¹è¯</h3>
        <p>ä½¿ç”¨å®Œæ•´AI Agentç³»ç»Ÿï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œè®°å¿†ç®¡ç†</p>
    </div>
    """, unsafe_allow_html=True)

    # åˆå§‹åŒ–å®Œæ•´Agent
    if 'agent' not in st.session_state and FULL_SYSTEM_AVAILABLE:
        try:
            with ui_components['loading'].spinner_with_text("åˆå§‹åŒ–å®Œæ•´AI Agentç³»ç»Ÿ...", "processing"):
                st.session_state.agent = MultiModalAgent()
                st.success("âœ… å®Œæ•´AI Agentç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            st.error(f"å®Œæ•´ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            optimized_chat_interface(client)
            return

    # åˆå§‹åŒ–å¯¹è¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå¯¹è¯å†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant" and "processing_time" in message:
                st.caption(f"â±ï¸ å¤„ç†æ—¶é—´: {message['processing_time']:.2f}ç§’")
                if "cached" in message and message["cached"]:
                    st.caption("ğŸš€ æ¥è‡ªç¼“å­˜")

    # ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ç”ŸæˆAIå›å¤ï¼ˆä½¿ç”¨å®Œæ•´Agent + ç¼“å­˜ï¼‰
        with st.chat_message("assistant"):
            process_enhanced_chat_input(client, prompt)

def optimized_chat_interface(client):
    """ä¼˜åŒ–ç‰ˆå¯¹è¯ç•Œé¢ï¼ˆç®€åŒ–ç³»ç»Ÿ+ç¼“å­˜ï¼‰"""
    st.markdown("""
    <div class="feature-card fade-in">
        <h3>ğŸ’¬ æ™ºèƒ½å¯¹è¯</h3>
        <p>ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–çš„å¯¹è¯ç³»ç»Ÿï¼Œæå‡å“åº”é€Ÿåº¦</p>
    </div>
    """, unsafe_allow_html=True)

    # åˆå§‹åŒ–å¯¹è¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå¯¹è¯å†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant":
                if "processing_time" in message:
                    st.caption(f"â±ï¸ å¤„ç†æ—¶é—´: {message['processing_time']:.2f}ç§’")
                if "cached" in message and message["cached"]:
                    st.caption("ğŸš€ æ¥è‡ªç¼“å­˜")

    # ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ç”ŸæˆAIå›å¤ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰
        with st.chat_message("assistant"):
            process_optimized_chat_input(client, prompt)

def process_enhanced_chat_input(client, prompt):
    """å¤„ç†å¢å¼ºç‰ˆèŠå¤©è¾“å…¥"""
    model = os.getenv("ARK_MODEL", "ep-20250506230532-w7rdw")

    # æ£€æŸ¥ç¼“å­˜
    cached_response = None
    if OPTIMIZATION_AVAILABLE:
        cached_response = cache_manager.get_cached_response(prompt, model, 0.7, 500)

    if cached_response:
        # ä½¿ç”¨ç¼“å­˜å“åº”
        st.markdown(cached_response)
        st.session_state.messages.append({
            "role": "assistant",
            "content": cached_response,
            "cached": True,
            "processing_time": 0.1
        })
    else:
        # ä½¿ç”¨å®Œæ•´Agentå¤„ç†
        with ui_components['loading'].spinner_with_text("AI Agentæ­£åœ¨æ€è€ƒ...", "thinking"):
            try:
                start_time = time.time()

                input_data = {
                    "type": "text",
                    "content": prompt
                }

                result = asyncio.run(st.session_state.agent.process_input(input_data))
                assistant_response = result.get('response', 'å¤„ç†å¤±è´¥')
                processing_time = time.time() - start_time

                st.markdown(assistant_response)

                # ç¼“å­˜å“åº”
                if OPTIMIZATION_AVAILABLE:
                    cache_manager.cache_response(prompt, model, 0.7, 500, assistant_response)

                # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": assistant_response,
                    "processing_time": processing_time,
                    "cached": False
                })

            except Exception as e:
                error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_msg
                })

def process_optimized_chat_input(client, prompt):
    """å¤„ç†ä¼˜åŒ–ç‰ˆèŠå¤©è¾“å…¥"""
    model = os.getenv("ARK_MODEL", "ep-20250506230532-w7rdw")

    # æ£€æŸ¥ç¼“å­˜
    cached_response = None
    if OPTIMIZATION_AVAILABLE:
        cached_response = cache_manager.get_cached_response(prompt, model, 0.7, 500)

    if cached_response:
        # ä½¿ç”¨ç¼“å­˜å“åº”
        st.markdown(cached_response)
        st.session_state.messages.append({
            "role": "assistant",
            "content": cached_response,
            "cached": True,
            "processing_time": 0.1
        })
    else:
        # ä½¿ç”¨APIè°ƒç”¨
        with st.spinner("ğŸ¤” AIæ­£åœ¨æ€è€ƒ..."):
            try:
                start_time = time.time()

                # æ„å»ºæ¶ˆæ¯å†å²
                messages = [
                    {"role": "system", "content": "ä½ æ˜¯è±†åŒ…AIåŠ©æ‰‹ï¼Œä¸€ä¸ªæ™ºèƒ½ã€å‹å¥½ã€æœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"}
                ]
                messages.extend(st.session_state.messages)

                # è°ƒç”¨API
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=500
                )

                assistant_response = response.choices[0].message.content
                processing_time = time.time() - start_time

                st.markdown(assistant_response)

                # ç¼“å­˜å“åº”
                if OPTIMIZATION_AVAILABLE:
                    cache_manager.cache_response(prompt, model, 0.7, 500, assistant_response)

                # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": assistant_response,
                    "processing_time": processing_time,
                    "cached": False
                })

            except Exception as e:
                st.error(f"ç”Ÿæˆå›å¤å¤±è´¥: {e}")

def enhanced_file_interface(client):
    """å¢å¼ºç‰ˆæ–‡ä»¶å¤„ç†ç•Œé¢"""
    st.markdown("""
    <div class="feature-card fade-in">
        <h3>ğŸ“ æ–‡ä»¶å¤„ç†</h3>
        <p>æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼çš„æ™ºèƒ½è§£æå’Œæ‰¹é‡å¤„ç†</p>
    </div>
    """, unsafe_allow_html=True)

    # ä½¿ç”¨å¢å¼ºçš„æ–‡ä»¶ä¸Šä¼ å™¨
    if OPTIMIZATION_AVAILABLE:
        uploaded_file = ui_components['interactive'].enhanced_file_uploader(
            label="é€‰æ‹©æ–‡ä»¶",
            accepted_types=['txt', 'md', 'json', 'csv', 'pdf', 'docx', 'xlsx'],
            max_size_mb=20,
            help_text="æ”¯æŒæ™ºèƒ½è§£æå’Œæ‰¹é‡å¤„ç†ï¼Œæ–‡ä»¶å°†è¢«å®‰å…¨å¤„ç†"
        )
    else:
        uploaded_file = st.file_uploader(
            "é€‰æ‹©æ–‡ä»¶",
            type=['txt', 'md', 'json', 'csv', 'pdf', 'docx', 'xlsx']
        )

    if uploaded_file is not None:
        # æ˜¾ç¤ºå¤„ç†é€‰é¡¹
        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("ğŸ“– æ™ºèƒ½è§£æ", key="parse_file"):
                process_file_with_progress(uploaded_file, "è§£æ")

        with col2:
            if st.button("ğŸ“Š å†…å®¹æ‘˜è¦", key="summarize_file"):
                process_file_with_progress(uploaded_file, "æ‘˜è¦")

        with col3:
            if st.button("ğŸ” å…³é”®ä¿¡æ¯æå–", key="extract_file"):
                process_file_with_progress(uploaded_file, "æå–")

def optimized_file_interface(client):
    """ä¼˜åŒ–ç‰ˆæ–‡ä»¶å¤„ç†ç•Œé¢"""
    st.markdown("""
    <div class="feature-card fade-in">
        <h3>ğŸ“ æ–‡ä»¶å¤„ç†</h3>
        <p>ä¼˜åŒ–çš„æ–‡ä»¶å¤„ç†ï¼Œæ”¯æŒç¼“å­˜å’Œè¿›åº¦æ˜¾ç¤º</p>
    </div>
    """, unsafe_allow_html=True)

    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader(
        "é€‰æ‹©æ–‡ä»¶",
        type=['txt', 'md', 'json', 'csv'],
        help="æ”¯æŒæ–‡æœ¬æ–‡ä»¶ã€Markdownã€JSONã€CSVæ ¼å¼"
    )

    if uploaded_file is not None:
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        st.success(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {uploaded_file.name}")
        st.info(f"æ–‡ä»¶å¤§å°: {uploaded_file.size} å­—èŠ‚")

        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            content = read_file_content(uploaded_file)

            # æ˜¾ç¤ºæ–‡ä»¶å†…å®¹é¢„è§ˆ
            st.subheader("ğŸ“„ æ–‡ä»¶å†…å®¹é¢„è§ˆ")
            st.text_area("å†…å®¹", value=content[:1000] + "..." if len(content) > 1000 else content, height=200)

            # æ–‡ä»¶åˆ†æé€‰é¡¹
            st.subheader("ğŸ” æ–‡ä»¶åˆ†æ")

            col1, col2 = st.columns(2)

            with col1:
                if st.button("ğŸ“Š å†…å®¹æ‘˜è¦"):
                    analyze_file_with_cache(client, content, "è¯·å¯¹ä»¥ä¸‹æ–‡ä»¶å†…å®¹è¿›è¡Œæ‘˜è¦æ€»ç»“")

            with col2:
                if st.button("ğŸ” å…³é”®ä¿¡æ¯æå–"):
                    analyze_file_with_cache(client, content, "è¯·æå–ä»¥ä¸‹æ–‡ä»¶å†…å®¹ä¸­çš„å…³é”®ä¿¡æ¯")

        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")

def enhanced_tools_interface(client):
    """å¢å¼ºç‰ˆå·¥å…·ç®±ç•Œé¢"""
    st.markdown("""
    <div class="feature-card fade-in">
        <h3>ğŸ§® AIå·¥å…·ç®±</h3>
        <p>å®Œæ•´å·¥å…·é“¾ç³»ç»Ÿï¼Œæ”¯æŒå¼‚æ­¥å¤„ç†å’Œæ‰¹é‡æ“ä½œ</p>
    </div>
    """, unsafe_allow_html=True)

    if 'agent' not in st.session_state:
        st.error("Agentæœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨å·¥å…·")
        return

    # å·¥å…·ç±»åˆ«é€‰æ‹©
    tool_category = st.selectbox(
        "é€‰æ‹©å·¥å…·ç±»åˆ«",
        ["ğŸ’­ åˆ›æ„å†™ä½œ", "ğŸ”¤ æ–‡æœ¬ç¿»è¯‘", "ğŸ“ å†…å®¹æ”¹å†™", "ğŸ§® æ•°å­¦è®¡ç®—", "ğŸ“Š æ•°æ®åˆ†æ", "ğŸ” ç½‘ç»œæœç´¢", "ğŸ’» ä»£ç æ‰§è¡Œ"],
        key="enhanced_tool_category"
    )

    if tool_category == "ğŸ’­ åˆ›æ„å†™ä½œ":
        enhanced_creative_writing_tool(client)
    elif tool_category == "ğŸ”¤ æ–‡æœ¬ç¿»è¯‘":
        enhanced_translation_tool(client)
    elif tool_category == "ğŸ“ å†…å®¹æ”¹å†™":
        enhanced_rewriting_tool(client)
    elif tool_category == "ğŸ§® æ•°å­¦è®¡ç®—":
        enhanced_math_tool(client)
    elif tool_category == "ğŸ“Š æ•°æ®åˆ†æ":
        enhanced_data_analysis_tool(client)

def optimized_tools_interface(client):
    """ä¼˜åŒ–ç‰ˆå·¥å…·ç®±ç•Œé¢"""
    st.markdown("""
    <div class="feature-card fade-in">
        <h3>ğŸ§® AIå·¥å…·ç®±</h3>
        <p>ç¼“å­˜ä¼˜åŒ–çš„å·¥å…·ç³»ç»Ÿï¼Œæå‡å¤„ç†é€Ÿåº¦</p>
    </div>
    """, unsafe_allow_html=True)

    # å·¥å…·é€‰æ‹©
    tool_option = st.selectbox(
        "é€‰æ‹©å·¥å…·",
        ["ğŸ’­ åˆ›æ„å†™ä½œ", "ğŸ”¤ æ–‡æœ¬ç¿»è¯‘", "ğŸ“ å†…å®¹æ”¹å†™", "ğŸ§® æ•°å­¦è®¡ç®—", "ğŸ“Š æ•°æ®åˆ†æ"],
        key="optimized_tool_option"
    )

    if tool_option == "ğŸ’­ åˆ›æ„å†™ä½œ":
        optimized_creative_writing_tool(client)
    elif tool_option == "ğŸ”¤ æ–‡æœ¬ç¿»è¯‘":
        optimized_translation_tool(client)
    elif tool_option == "ğŸ“ å†…å®¹æ”¹å†™":
        optimized_rewriting_tool(client)
    elif tool_option == "ğŸ§® æ•°å­¦è®¡ç®—":
        optimized_math_tool(client)
    elif tool_option == "ğŸ“Š æ•°æ®åˆ†æ":
        optimized_data_analysis_tool(client)

def memory_interface():
    """è®°å¿†ç®¡ç†ç•Œé¢"""
    st.markdown("""
    <div class="feature-card fade-in">
        <h3>ğŸ§  è®°å¿†ç®¡ç†</h3>
        <p>æ™ºèƒ½è®°å¿†ç³»ç»Ÿï¼Œæ”¯æŒæœç´¢ã€ç»Ÿè®¡å’Œå¯¼å‡º</p>
    </div>
    """, unsafe_allow_html=True)

    if 'agent' not in st.session_state:
        st.error("Agentæœªåˆå§‹åŒ–ï¼Œæ— æ³•è®¿é—®è®°å¿†ç³»ç»Ÿ")
        return

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ğŸ” æœç´¢è®°å¿†")
        search_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯", key="memory_search")
        if st.button("æœç´¢", key="search_memory_btn") and search_query:
            search_memory_enhanced(search_query)

    with col2:
        st.subheader("ğŸ“Š è®°å¿†ç»Ÿè®¡")
        if st.button("æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯", key="memory_stats_btn"):
            show_memory_stats()

    st.subheader("ğŸ—‘ï¸ è®°å¿†ç®¡ç†")
    col3, col4 = st.columns(2)

    with col3:
        if st.button("æ¸…é™¤æ‰€æœ‰è®°å¿†", type="secondary", key="clear_memory_btn"):
            clear_all_memory()

    with col4:
        if st.button("å¯¼å‡ºè®°å¿†", type="secondary", key="export_memory_btn"):
            export_memory()

def performance_monitoring_interface():
    """æ€§èƒ½ç›‘æ§ç•Œé¢"""
    st.markdown("""
    <div class="feature-card fade-in">
        <h3>ğŸ“Š æ€§èƒ½ç›‘æ§</h3>
        <p>å®æ—¶ç›‘æ§ç³»ç»Ÿæ€§èƒ½å’Œèµ„æºä½¿ç”¨æƒ…å†µ</p>
    </div>
    """, unsafe_allow_html=True)

    if not OPTIMIZATION_AVAILABLE:
        st.warning("æ€§èƒ½ç›‘æ§åŠŸèƒ½éœ€è¦ä¼˜åŒ–ç»„ä»¶æ”¯æŒ")
        return

    # æ€§èƒ½æŒ‡æ ‡ä»ªè¡¨æ¿
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ğŸ’¾ å†…å­˜ç›‘æ§")
        memory_stats = memory_optimizer.get_optimization_stats()

        # å†…å­˜ä½¿ç”¨å›¾è¡¨
        if len(memory_stats.get('history', [])) > 1:
            import pandas as pd

            history_data = memory_stats['history'][-20:]  # æœ€è¿‘20æ¡è®°å½•
            df = pd.DataFrame(history_data)
            df['timestamp'] = pd.to_datetime(df['timestamp'])

            st.line_chart(df.set_index('timestamp')['usage_mb'])

        # å†…å­˜æŒ‡æ ‡
        ui_components['metrics'].performance_dashboard({
            'response_time': 0.5,  # ç¤ºä¾‹æ•°æ®
            'cache_hit_rate': cache_manager.get_cache_stats()['hit_rate'],
            'total_requests': cache_manager.get_cache_stats()['total_requests'],
            'error_rate': 0.1
        })

    with col2:
        st.subheader("ğŸš€ ç¼“å­˜ç›‘æ§")
        cache_stats = cache_manager.get_cache_stats()

        # ç¼“å­˜æŒ‡æ ‡
        st.metric("å‘½ä¸­ç‡", f"{cache_stats['hit_rate']:.1f}%")
        st.metric("ç¼“å­˜å¤§å°", f"{cache_stats['cache_size']}")
        st.metric("æ€»è¯·æ±‚æ•°", cache_stats['total_requests'])

        # ç¼“å­˜è¯¦ç»†ä¿¡æ¯
        if st.button("æŸ¥çœ‹ç¼“å­˜è¯¦æƒ…", key="cache_details_btn"):
            cache_info = cache_manager.get_cache_info()
            st.json(cache_info)

    # ç³»ç»Ÿå»ºè®®
    st.subheader("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
    recommendations = memory_optimizer.get_memory_recommendations()
    for i, recommendation in enumerate(recommendations):
        st.info(f"{i+1}. {recommendation}")

# è¾…åŠ©å‡½æ•°
def process_file_with_progress(uploaded_file, action):
    """å¸¦è¿›åº¦æ˜¾ç¤ºçš„æ–‡ä»¶å¤„ç†"""
    if not OPTIMIZATION_AVAILABLE:
        st.error("éœ€è¦ä¼˜åŒ–ç»„ä»¶æ”¯æŒ")
        return

    # åˆ›å»ºè¿›åº¦è·Ÿè¸ª
    task_id = f"file_{action}_{int(time.time())}"
    progress_tracker.start_progress(task_id, 3, f"{action}æ–‡ä»¶ä¸­...")

    # æ­¥éª¤1ï¼šè¯»å–æ–‡ä»¶
    progress_tracker.update_progress(task_id, 1, "è¯»å–æ–‡ä»¶...")
    progress_tracker.render_progress_bar(task_id)

    try:
        content = read_file_content(uploaded_file)

        # æ­¥éª¤2ï¼šå¤„ç†æ–‡ä»¶
        progress_tracker.update_progress(task_id, 2, f"{action}å¤„ç†ä¸­...")
        progress_tracker.render_progress_bar(task_id)

        if 'agent' in st.session_state:
            input_data = {
                "type": "file",
                "content": content
            }
            result = asyncio.run(st.session_state.agent.process_input(input_data))

            # æ­¥éª¤3ï¼šå®Œæˆ
            progress_tracker.update_progress(task_id, 3, "å¤„ç†å®Œæˆ")
            progress_tracker.render_progress_bar(task_id)
            progress_tracker.complete_progress(task_id)

            st.success(f"âœ… {action}å®Œæˆ")
            st.markdown("### å¤„ç†ç»“æœ")
            st.markdown(result.get('response', 'å¤„ç†å¤±è´¥'))
        else:
            st.error("Agentæœªåˆå§‹åŒ–")

    except Exception as e:
        st.error(f"{action}å¤±è´¥: {str(e)}")

def read_file_content(uploaded_file):
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    if uploaded_file.type == "text/plain" or uploaded_file.name.endswith('.md'):
        return uploaded_file.read().decode('utf-8')
    elif uploaded_file.name.endswith('.json'):
        import json
        return json.dumps(json.load(uploaded_file), ensure_ascii=False, indent=2)
    elif uploaded_file.name.endswith('.csv'):
        return uploaded_file.read().decode('utf-8')
    else:
        return str(uploaded_file.read())

def analyze_file_with_cache(client, content, instruction):
    """å¸¦ç¼“å­˜çš„æ–‡ä»¶åˆ†æ"""
    model = os.getenv("ARK_MODEL", "ep-20250506230532-w7rdw")

    # æ£€æŸ¥ç¼“å­˜
    cache_key = f"{instruction}:{content[:100]}"
    cached_response = None

    if OPTIMIZATION_AVAILABLE:
        cached_response = cache_manager.get_cached_response(cache_key, model, 0.3, 800)

    if cached_response:
        st.success("âœ… åˆ†æå®Œæˆï¼ˆæ¥è‡ªç¼“å­˜ï¼‰")
        st.markdown("### åˆ†æç»“æœ")
        st.markdown(cached_response)
    else:
        with st.spinner("AIæ­£åœ¨åˆ†ææ–‡ä»¶..."):
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚"},
                        {"role": "user", "content": f"{instruction}:\n\n{content[:3000]}"}
                    ],
                    temperature=0.3,
                    max_tokens=800
                )

                result = response.choices[0].message.content

                # ç¼“å­˜ç»“æœ
                if OPTIMIZATION_AVAILABLE:
                    cache_manager.cache_response(cache_key, model, 0.3, 800, result)

                st.success("âœ… åˆ†æå®Œæˆ")
                st.markdown("### åˆ†æç»“æœ")
                st.markdown(result)

            except Exception as e:
                st.error(f"åˆ†æå¤±è´¥: {e}")

# ä¼˜åŒ–ç‰ˆå·¥å…·å‡½æ•°
def optimized_creative_writing_tool(client):
    """ä¼˜åŒ–ç‰ˆåˆ›æ„å†™ä½œå·¥å…·"""
    st.subheader("ğŸ’­ åˆ›æ„å†™ä½œåŠ©æ‰‹")

    # ä½¿ç”¨æ™ºèƒ½æ–‡æœ¬è¾“å…¥
    if OPTIMIZATION_AVAILABLE:
        topic = ui_components['interactive'].smart_text_input(
            "ä¸»é¢˜æˆ–å…³é”®è¯",
            placeholder="è¯·è¾“å…¥åˆ›ä½œä¸»é¢˜...",
            max_chars=200,
            suggestions=["ç§‘æŠ€åˆ›æ–°", "ç¯ä¿ç”Ÿæ´»", "äººå·¥æ™ºèƒ½", "æœªæ¥ä¸–ç•Œ", "æ•™è‚²æ”¹é©"]
        )
    else:
        topic = st.text_input("ä¸»é¢˜æˆ–å…³é”®è¯")

    col1, col2, col3 = st.columns(3)

    with col1:
        writing_type = st.selectbox("å†™ä½œç±»å‹", ["æ–‡ç« ", "æ•…äº‹", "è¯—æ­Œ", "å¹¿å‘Šæ–‡æ¡ˆ", "é‚®ä»¶"])

    with col2:
        style = st.selectbox("å†™ä½œé£æ ¼", ["æ­£å¼", "è½»æ¾", "å¹½é»˜", "ä¸“ä¸š", "åˆ›æ„"])

    with col3:
        length = st.selectbox("é•¿åº¦", ["ç®€çŸ­", "ä¸­ç­‰", "è¯¦ç»†"])

    if st.button("âœ¨ å¼€å§‹åˆ›ä½œ", key="optimized_creative_btn") and topic:
        create_content_with_cache(client, writing_type, topic, style, length)

def create_content_with_cache(client, writing_type, topic, style, length):
    """å¸¦ç¼“å­˜çš„å†…å®¹åˆ›ä½œ"""
    model = os.getenv("ARK_MODEL", "ep-20250506230532-w7rdw")
    prompt = f"è¯·å†™ä¸€ç¯‡{style}é£æ ¼çš„{writing_type}ï¼Œä¸»é¢˜æ˜¯ï¼š{topic}ã€‚é•¿åº¦è¦æ±‚ï¼š{length}ã€‚"

    # æ£€æŸ¥ç¼“å­˜
    cached_response = None
    if OPTIMIZATION_AVAILABLE:
        cached_response = cache_manager.get_cached_response(prompt, model, 0.8, 1000)

    if cached_response:
        st.success("âœ… åˆ›ä½œå®Œæˆï¼ˆæ¥è‡ªç¼“å­˜ï¼‰")
        st.markdown("### åˆ›ä½œç»“æœ")
        st.markdown(cached_response)
    else:
        with st.spinner("AIæ­£åœ¨åˆ›ä½œ..."):
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆ›æ„å†™ä½œåŠ©æ‰‹ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.8,
                    max_tokens=1000
                )

                result = response.choices[0].message.content

                # ç¼“å­˜ç»“æœ
                if OPTIMIZATION_AVAILABLE:
                    cache_manager.cache_response(prompt, model, 0.8, 1000, result)

                st.success("âœ… åˆ›ä½œå®Œæˆ")
                st.markdown("### åˆ›ä½œç»“æœ")
                st.markdown(result)

            except Exception as e:
                st.error(f"åˆ›ä½œå¤±è´¥: {e}")

# å…¶ä»–ä¼˜åŒ–å·¥å…·å‡½æ•°çš„å ä½ç¬¦
def optimized_translation_tool(client):
    """ä¼˜åŒ–ç‰ˆç¿»è¯‘å·¥å…·"""
    st.info("ç¿»è¯‘å·¥å…·ä¼˜åŒ–ç‰ˆå¼€å‘ä¸­...")

def optimized_rewriting_tool(client):
    """ä¼˜åŒ–ç‰ˆæ”¹å†™å·¥å…·"""
    st.info("æ”¹å†™å·¥å…·ä¼˜åŒ–ç‰ˆå¼€å‘ä¸­...")

def optimized_math_tool(client):
    """ä¼˜åŒ–ç‰ˆæ•°å­¦å·¥å…·"""
    st.info("æ•°å­¦å·¥å…·ä¼˜åŒ–ç‰ˆå¼€å‘ä¸­...")

def optimized_data_analysis_tool(client):
    """ä¼˜åŒ–ç‰ˆæ•°æ®åˆ†æå·¥å…·"""
    st.info("æ•°æ®åˆ†æå·¥å…·ä¼˜åŒ–ç‰ˆå¼€å‘ä¸­...")

# å¢å¼ºç‰ˆå·¥å…·å‡½æ•°çš„å ä½ç¬¦
def enhanced_creative_writing_tool(client):
    """å¢å¼ºç‰ˆåˆ›æ„å†™ä½œå·¥å…·"""
    st.info("å¢å¼ºç‰ˆåˆ›æ„å†™ä½œå·¥å…·å¼€å‘ä¸­...")

def enhanced_translation_tool(client):
    """å¢å¼ºç‰ˆç¿»è¯‘å·¥å…·"""
    st.info("å¢å¼ºç‰ˆç¿»è¯‘å·¥å…·å¼€å‘ä¸­...")

def enhanced_rewriting_tool(client):
    """å¢å¼ºç‰ˆæ”¹å†™å·¥å…·"""
    st.info("å¢å¼ºç‰ˆæ”¹å†™å·¥å…·å¼€å‘ä¸­...")

def enhanced_math_tool(client):
    """å¢å¼ºç‰ˆæ•°å­¦å·¥å…·"""
    st.info("å¢å¼ºç‰ˆæ•°å­¦å·¥å…·å¼€å‘ä¸­...")

def enhanced_data_analysis_tool(client):
    """å¢å¼ºç‰ˆæ•°æ®åˆ†æå·¥å…·"""
    st.info("å¢å¼ºç‰ˆæ•°æ®åˆ†æå·¥å…·å¼€å‘ä¸­...")

# è®°å¿†ç®¡ç†å‡½æ•°
def search_memory_enhanced(query):
    """å¢å¼ºç‰ˆè®°å¿†æœç´¢"""
    with st.spinner("æœç´¢è®°å¿†ä¸­..."):
        try:
            results = asyncio.run(st.session_state.agent.search_memory(query))

            if results:
                st.success(f"æ‰¾åˆ° {len(results)} æ¡ç›¸å…³è®°å¿†")
                for i, result in enumerate(results):
                    with st.expander(f"è®°å¿† {i+1}"):
                        st.text(result.page_content)
                        if hasattr(result, 'metadata') and result.metadata:
                            st.json(result.metadata)
            else:
                st.info("æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")

        except Exception as e:
            st.error(f"æœç´¢å¤±è´¥: {str(e)}")

def show_memory_stats():
    """æ˜¾ç¤ºè®°å¿†ç»Ÿè®¡"""
    try:
        stats = st.session_state.agent.memory_manager.get_memory_stats()
        st.json(stats)
    except Exception as e:
        st.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

def clear_all_memory():
    """æ¸…é™¤æ‰€æœ‰è®°å¿†"""
    if st.button("ç¡®è®¤æ¸…é™¤", type="primary", key="confirm_clear_memory"):
        try:
            asyncio.run(st.session_state.agent.clear_memory())
            st.success("âœ… æ‰€æœ‰è®°å¿†å·²æ¸…é™¤")
        except Exception as e:
            st.error(f"æ¸…é™¤å¤±è´¥: {str(e)}")

def export_memory():
    """å¯¼å‡ºè®°å¿†"""
    st.info("è®°å¿†å¯¼å‡ºåŠŸèƒ½å¼€å‘ä¸­...")

# ä¸»å‡½æ•°å¯ä»¥è¢«app.pyå¯¼å…¥å’Œè°ƒç”¨
if __name__ == "__main__":
    main()
