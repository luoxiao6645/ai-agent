# ğŸ¢ ä¼ä¸šç‰ˆéƒ¨ç½²æŒ‡å— - ç¬¬å…­é˜¶æ®µ

## ğŸ“‹ æ¦‚è¿°

ç¬¬å…­é˜¶æ®µå¼•å…¥äº†å¾®æœåŠ¡æ¶æ„ã€æ’ä»¶ç³»ç»Ÿã€æœ¬åœ°æ¨¡å‹éƒ¨ç½²å’Œå¤šè¯­è¨€æ”¯æŒï¼Œæä¾›äº†å®Œæ•´çš„ä¼ä¸šçº§AI Agentè§£å†³æ–¹æ¡ˆã€‚

## ğŸ†• ç¬¬å…­é˜¶æ®µæ–°åŠŸèƒ½

### ğŸ”— å¾®æœåŠ¡æ¶æ„
- **æœåŠ¡æ³¨å†Œä¸­å¿ƒ**: è‡ªåŠ¨æœåŠ¡å‘ç°å’Œå¥åº·æ£€æŸ¥
- **è´Ÿè½½å‡è¡¡**: æ™ºèƒ½è¯·æ±‚åˆ†å‘
- **æœåŠ¡æ²»ç†**: æœåŠ¡çŠ¶æ€ç›‘æ§å’Œç®¡ç†

### ğŸ”Œ æ’ä»¶ç³»ç»Ÿ
- **åŠ¨æ€åŠ è½½**: è¿è¡Œæ—¶åŠ è½½/å¸è½½æ’ä»¶
- **ç¬¬ä¸‰æ–¹æ‰©å±•**: æ”¯æŒè‡ªå®šä¹‰åŠŸèƒ½æ’ä»¶
- **é’©å­æœºåˆ¶**: çµæ´»çš„äº‹ä»¶å¤„ç†

### ğŸ¤– æœ¬åœ°æ¨¡å‹éƒ¨ç½²
- **HuggingFaceæ¨¡å‹**: æ”¯æŒTransformersç”Ÿæ€
- **Ollamaé›†æˆ**: æœ¬åœ°å¤§æ¨¡å‹æ¨ç†
- **æ¨¡å‹ç®¡ç†**: åŠ¨æ€åŠ è½½/å¸è½½æ¨¡å‹

### ğŸŒ å¤šè¯­è¨€æ”¯æŒ
- **å›½é™…åŒ–ç•Œé¢**: ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡
- **åŠ¨æ€åˆ‡æ¢**: è¿è¡Œæ—¶è¯­è¨€åˆ‡æ¢
- **å¯æ‰©å±•**: æ”¯æŒæ·»åŠ æ–°è¯­è¨€

## ğŸš€ éƒ¨ç½²æ–¹å¼

### æ–¹å¼ä¸€ï¼šä¼ä¸šç‰ˆå®Œæ•´éƒ¨ç½²

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/luoxiao6645/ai-agent.git
cd ai-agent

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# é…ç½®ç¯å¢ƒå˜é‡
python secure_setup.py

# å¯åŠ¨ä¼ä¸šç‰ˆåº”ç”¨
python enterprise_app.py
```

### æ–¹å¼äºŒï¼šDockerä¼ä¸šç‰ˆéƒ¨ç½²

```bash
# æ„å»ºä¼ä¸šç‰ˆé•œåƒ
docker build -t ai-agent-enterprise -f Dockerfile.enterprise .

# å¯åŠ¨ä¼ä¸šç‰ˆæœåŠ¡
docker-compose -f docker-compose.enterprise.yml up -d
```

### æ–¹å¼ä¸‰ï¼šKuberneteséƒ¨ç½²

```bash
# åº”ç”¨Kubernetesé…ç½®
kubectl apply -f k8s/

# æ£€æŸ¥éƒ¨ç½²çŠ¶æ€
kubectl get pods -n ai-agent
```

## ğŸŒ æœåŠ¡ç«¯ç‚¹

### ä¸»è¦æœåŠ¡
- **ğŸŒ ä¸»åº”ç”¨**: http://localhost:8501
- **ğŸ¥ å¥åº·æ£€æŸ¥**: http://localhost:8080/health
- **ğŸ“Š ç³»ç»ŸæŒ‡æ ‡**: http://localhost:8080/metrics
- **ğŸ”— æœåŠ¡æ³¨å†Œ**: http://localhost:8080/registry

### å¾®æœåŠ¡ç«¯ç‚¹
- **ğŸ“ˆ Prometheus**: http://localhost:9090
- **ğŸ“Š Grafana**: http://localhost:3000
- **ğŸ”— GraphQL API**: http://localhost:8000/graphql
- **ğŸ“± ç§»åŠ¨ç«¯API**: http://localhost:8001/docs

### ç®¡ç†ç«¯ç‚¹
- **ğŸ”Œ æ’ä»¶ç®¡ç†**: http://localhost:8080/plugins
- **ğŸ¤– æ¨¡å‹ç®¡ç†**: http://localhost:8080/models
- **ğŸŒ è¯­è¨€è®¾ç½®**: http://localhost:8080/i18n

## âš™ï¸ ç¯å¢ƒå˜é‡é…ç½®

### åŸºç¡€é…ç½®
```bash
# APIå¯†é’¥
ARK_API_KEY=your_volcano_engine_ark_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

# æœåŠ¡ç«¯å£
STREAMLIT_PORT=8501
HEALTH_CHECK_PORT=8080
PROMETHEUS_PORT=8090
GRAPHQL_PORT=8000
MOBILE_API_PORT=8001
```

### ä¼ä¸šç‰ˆé…ç½®
```bash
# å¾®æœåŠ¡é…ç½®
MICROSERVICES_ENABLED=true
SERVICE_REGISTRY_PORT=8080
HEALTH_CHECK_INTERVAL=30

# æ’ä»¶é…ç½®
PLUGINS_ENABLED=true
PLUGINS_DIR=plugins
AUTO_LOAD_PLUGINS=true

# æœ¬åœ°æ¨¡å‹é…ç½®
LOCAL_MODELS_ENABLED=true
MODELS_DIR=local_models
DEFAULT_DEVICE=auto

# å›½é™…åŒ–é…ç½®
I18N_ENABLED=true
DEFAULT_LANGUAGE=zh_CN
LOCALES_DIR=i18n/locales
```

## ğŸ”Œ æ’ä»¶å¼€å‘

### åˆ›å»ºæ’ä»¶

1. **åˆ›å»ºæ’ä»¶ç›®å½•**
```bash
mkdir plugins/my_plugin
cd plugins/my_plugin
```

2. **åˆ›å»ºæ’ä»¶æ–‡ä»¶**
```python
# plugins/my_plugin/plugin.py
from plugins.plugin_manager import PluginInterface, PluginInfo

class MyPlugin(PluginInterface):
    def get_info(self) -> PluginInfo:
        return PluginInfo(
            name="my_plugin",
            version="1.0.0",
            description="æˆ‘çš„è‡ªå®šä¹‰æ’ä»¶",
            author="Your Name",
            dependencies=[],
            entry_point="MyPlugin",
            config_schema={}
        )
    
    def initialize(self, config):
        print("æ’ä»¶åˆå§‹åŒ–")
        return True
    
    def execute(self, *args, **kwargs):
        return "æ’ä»¶æ‰§è¡Œç»“æœ"
    
    def cleanup(self):
        print("æ’ä»¶æ¸…ç†")
        return True
```

3. **åŠ è½½æ’ä»¶**
```python
from plugins.plugin_manager import get_plugin_manager

plugin_manager = get_plugin_manager()
plugin_manager.load_plugin("my_plugin")
```

### æ’ä»¶é…ç½®

```json
{
  "my_plugin": {
    "enabled": true,
    "config": {
      "api_key": "your_api_key",
      "timeout": 30
    }
  }
}
```

## ğŸ¤– æœ¬åœ°æ¨¡å‹éƒ¨ç½²

### æ”¯æŒçš„æ¨¡å‹ç±»å‹

#### HuggingFaceæ¨¡å‹
```python
from local_models.model_manager import get_model_manager

model_manager = get_model_manager()

# æ³¨å†Œæ¨¡å‹
model_manager.register_model(
    name="chatglm3-6b",
    model_type="huggingface",
    model_path="THUDM/chatglm3-6b",
    tokenizer_path="THUDM/chatglm3-6b"
)

# åŠ è½½æ¨¡å‹
model_manager.load_model("chatglm3-6b")

# ç”Ÿæˆæ–‡æœ¬
response = model_manager.generate_text(
    "chatglm3-6b", 
    "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"
)
```

#### Ollamaæ¨¡å‹
```bash
# å®‰è£…Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# ä¸‹è½½æ¨¡å‹
ollama pull llama2

# æ³¨å†Œåˆ°ç³»ç»Ÿ
python -c "
from local_models.model_manager import get_model_manager
manager = get_model_manager()
manager.register_model('llama2', 'ollama', 'llama2')
manager.load_model('llama2')
"
```

### æ¨¡å‹é…ç½®

```json
{
  "models": [
    {
      "name": "chatglm3-6b",
      "model_type": "huggingface",
      "model_path": "THUDM/chatglm3-6b",
      "device": "cuda",
      "max_length": 2048,
      "temperature": 0.7
    },
    {
      "name": "llama2",
      "model_type": "ollama", 
      "model_path": "llama2",
      "device": "cpu",
      "max_length": 4096,
      "temperature": 0.8
    }
  ]
}
```

## ğŸŒ å¤šè¯­è¨€é…ç½®

### æ·»åŠ æ–°è¯­è¨€

1. **åˆ›å»ºè¯­è¨€æ–‡ä»¶**
```bash
# åˆ›å»ºéŸ©æ–‡ç¿»è¯‘
touch i18n/locales/ko_KR.json
```

2. **æ·»åŠ ç¿»è¯‘å†…å®¹**
```json
{
  "app": {
    "title": "ì§€ëŠ¥í˜• ë©€í‹°ëª¨ë‹¬ AI ì—ì´ì „íŠ¸",
    "welcome": "ì§€ëŠ¥í˜• AI ì–´ì‹œìŠ¤í„´íŠ¸ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!"
  },
  "navigation": {
    "chat": "ìŠ¤ë§ˆíŠ¸ ì±„íŒ…",
    "file_processing": "íŒŒì¼ ì²˜ë¦¬"
  }
}
```

3. **ä½¿ç”¨ç¿»è¯‘**
```python
from i18n.translator import set_language, t

# åˆ‡æ¢è¯­è¨€
set_language("ko_KR")

# è·å–ç¿»è¯‘
title = t("app.title")
```

### åŠ¨æ€è¯­è¨€åˆ‡æ¢

```python
import streamlit as st
from i18n.translator import get_available_languages, set_language, t

# è¯­è¨€é€‰æ‹©å™¨
languages = get_available_languages()
selected = st.selectbox(
    t("settings.language"),
    options=[lang["code"] for lang in languages],
    format_func=lambda x: next(lang["native"] for lang in languages if lang["code"] == x)
)

if selected:
    set_language(selected)
    st.rerun()
```

## ğŸ“Š ç›‘æ§å’Œç®¡ç†

### æœåŠ¡ç›‘æ§

```bash
# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
curl http://localhost:8080/health

# æŸ¥çœ‹æ³¨å†Œçš„æœåŠ¡
curl http://localhost:8080/registry

# æŸ¥çœ‹ç³»ç»ŸæŒ‡æ ‡
curl http://localhost:8080/metrics
```

### æ’ä»¶ç®¡ç†

```bash
# åˆ—å‡ºæ’ä»¶
curl http://localhost:8080/plugins

# åŠ è½½æ’ä»¶
curl -X POST http://localhost:8080/plugins/load -d '{"name": "weather_plugin"}'

# å¸è½½æ’ä»¶
curl -X POST http://localhost:8080/plugins/unload -d '{"name": "weather_plugin"}'
```

### æ¨¡å‹ç®¡ç†

```bash
# åˆ—å‡ºæ¨¡å‹
curl http://localhost:8080/models

# åŠ è½½æ¨¡å‹
curl -X POST http://localhost:8080/models/load -d '{"name": "chatglm3-6b"}'

# å¸è½½æ¨¡å‹
curl -X POST http://localhost:8080/models/unload -d '{"name": "chatglm3-6b"}'
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–

### æ¨¡å‹ä¼˜åŒ–

```python
# é‡åŒ–é…ç½®
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# æ³¨å†Œé‡åŒ–æ¨¡å‹
model_manager.register_model(
    name="chatglm3-6b-4bit",
    model_type="huggingface",
    model_path="THUDM/chatglm3-6b",
    quantization_config=quantization_config
)
```

### ç¼“å­˜ä¼˜åŒ–

```python
# æ¨¡å‹ç¼“å­˜
TRANSFORMERS_CACHE=/path/to/cache
HF_HOME=/path/to/hf_cache

# Redisç¼“å­˜
REDIS_URL=redis://localhost:6379
CACHE_TTL=3600
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### æ¨¡å‹åŠ è½½å¤±è´¥
```bash
# æ£€æŸ¥GPUå†…å­˜
nvidia-smi

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls -la local_models/

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
tail -f logs/app.jsonl
```

#### æ’ä»¶åŠ è½½å¤±è´¥
```bash
# æ£€æŸ¥æ’ä»¶ç›®å½•
ls -la plugins/

# éªŒè¯æ’ä»¶ä»£ç 
python -c "from plugins.my_plugin.plugin import MyPlugin; print('OK')"

# æŸ¥çœ‹æ’ä»¶æ—¥å¿—
grep "plugin" logs/app.jsonl
```

#### æœåŠ¡æ³¨å†Œå¤±è´¥
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tulpn | grep 8080

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8080/health

# é‡å¯æœåŠ¡æ³¨å†Œä¸­å¿ƒ
python -c "
from microservices.service_registry import get_service_registry
registry = get_service_registry()
registry.stop_health_check()
registry.start_health_check()
"
```

## ğŸ“ˆ æ‰©å±•éƒ¨ç½²

### é›†ç¾¤éƒ¨ç½²

```yaml
# docker-compose.cluster.yml
version: '3.8'
services:
  ai-agent-1:
    image: ai-agent-enterprise
    ports:
      - "8501:8501"
    environment:
      - INSTANCE_ID=1
      
  ai-agent-2:
    image: ai-agent-enterprise
    ports:
      - "8502:8501"
    environment:
      - INSTANCE_ID=2
      
  nginx:
    image: nginx
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
```

### äº‘åŸç”Ÿéƒ¨ç½²

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-agent-enterprise
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-agent
  template:
    metadata:
      labels:
        app: ai-agent
    spec:
      containers:
      - name: ai-agent
        image: ai-agent-enterprise:latest
        ports:
        - containerPort: 8501
        env:
        - name: ARK_API_KEY
          valueFrom:
            secretKeyRef:
              name: ai-agent-secrets
              key: ark-api-key
```

---

**ç¬¬å…­é˜¶æ®µä¸ºAI Agentç³»ç»Ÿæä¾›äº†å®Œæ•´çš„ä¼ä¸šçº§æ¶æ„å’Œæ‰©å±•èƒ½åŠ›ï¼** ğŸ¢
