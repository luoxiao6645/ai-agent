#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ€§èƒ½ä¼˜åŒ–æµ‹è¯•è„šæœ¬
ä¸ä¾èµ–å¤–éƒ¨åº“ï¼Œæµ‹è¯•æ ¸å¿ƒæ€§èƒ½ä¼˜åŒ–åŠŸèƒ½
"""

import asyncio
import time
import json
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SimplePerformanceTest:
    """ç®€åŒ–ç‰ˆæ€§èƒ½æµ‹è¯•"""
    
    def __init__(self):
        self.start_time = time.time()
        self.test_results = {}
        
    async def test_cache_performance(self):
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
        logger.info("ğŸ’¾ æµ‹è¯•ç¼“å­˜æ€§èƒ½...")
        
        from performance.cache_manager import cache_manager
        
        # æµ‹è¯•æ•°æ®
        test_data = {
            f"key_{i}": f"value_{i}" * 100 for i in range(100)
        }
        
        # å†™å…¥æµ‹è¯•
        start_time = time.time()
        for key, value in test_data.items():
            cache_manager.set(key, value, ttl=300)
        write_time = time.time() - start_time
        
        # è¯»å–æµ‹è¯•
        start_time = time.time()
        hits = 0
        for key in test_data.keys():
            if cache_manager.get(key) is not None:
                hits += 1
        read_time = time.time() - start_time
        
        # å‘½ä¸­ç‡æµ‹è¯•
        hit_rate = (hits / len(test_data)) * 100
        
        self.test_results["cache"] = {
            "write_time": write_time,
            "read_time": read_time,
            "hit_rate": hit_rate,
            "operations_per_second": len(test_data) / (write_time + read_time)
        }
        
        logger.info(f"   ç¼“å­˜å†™å…¥æ—¶é—´: {write_time:.3f}s")
        logger.info(f"   ç¼“å­˜è¯»å–æ—¶é—´: {read_time:.3f}s")
        logger.info(f"   ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}%")
    
    async def test_database_simulation(self):
        """æµ‹è¯•æ•°æ®åº“æŸ¥è¯¢æ¨¡æ‹Ÿ"""
        logger.info("ğŸ’¾ æµ‹è¯•æ•°æ®åº“æŸ¥è¯¢æ¨¡æ‹Ÿ...")
        
        from performance.database_optimizer import record_database_query, get_database_optimization_report
        
        # æ¨¡æ‹Ÿå„ç§æŸ¥è¯¢
        queries = [
            ("SELECT * FROM users WHERE id = ?", 0.05),
            ("SELECT * FROM conversations WHERE session_id = ?", 0.08),
            ("INSERT INTO messages (content, timestamp) VALUES (?, ?)", 0.03),
            ("UPDATE users SET last_active = ? WHERE id = ?", 0.04),
            ("SELECT COUNT(*) FROM messages WHERE created_at > ?", 0.12),
            ("SELECT * FROM large_table WHERE complex_condition = ?", 1.5),  # æ…¢æŸ¥è¯¢
        ]
        
        start_time = time.time()
        
        for i in range(50):
            query, base_time = queries[i % len(queries)]
            execution_time = base_time + (i % 3) * 0.01
            success = i % 20 != 19  # 5%å¤±è´¥ç‡
            
            record_database_query(
                query=query,
                execution_time=execution_time,
                rows_affected=1 if "SELECT" not in query else i % 10,
                success=success,
                error_message=None if success else "Simulated error"
            )
        
        total_time = time.time() - start_time
        
        # è·å–ä¼˜åŒ–æŠ¥å‘Š
        db_report = get_database_optimization_report()
        
        self.test_results["database"] = {
            "total_queries": 50,
            "simulation_time": total_time,
            "optimization_report": db_report
        }
        
        logger.info(f"   æ•°æ®åº“æŸ¥è¯¢æ¨¡æ‹Ÿæ—¶é—´: {total_time:.3f}s")
        logger.info(f"   æ…¢æŸ¥è¯¢æ•°é‡: {len(db_report.get('top_slow_queries', []))}")
    
    async def test_api_simulation(self):
        """æµ‹è¯•APIæ€§èƒ½æ¨¡æ‹Ÿ"""
        logger.info("ğŸŒ æµ‹è¯•APIæ€§èƒ½æ¨¡æ‹Ÿ...")
        
        from performance.api_optimizer import record_api_request, get_api_performance_summary
        
        # æ¨¡æ‹ŸAPIç«¯ç‚¹
        endpoints = [
            ("/api/v1/chat", "POST", 0.2),
            ("/api/v1/tools", "GET", 0.1),
            ("/api/v1/memory", "GET", 0.15),
            ("/api/v1/status", "GET", 0.05),
            ("/api/v1/upload", "POST", 0.8),
        ]
        
        start_time = time.time()
        
        for i in range(100):
            endpoint, method, base_time = endpoints[i % len(endpoints)]
            response_time = base_time + (i % 5) * 0.02
            status_code = 200 if i % 15 != 14 else 500  # ~7%é”™è¯¯ç‡
            
            record_api_request(
                endpoint=endpoint,
                method=method,
                response_time=response_time,
                status_code=status_code,
                request_size=1024 + (i % 10) * 100,
                response_size=2048 + (i % 20) * 200
            )
        
        total_time = time.time() - start_time
        
        # è·å–æ€§èƒ½æ‘˜è¦
        api_summary = get_api_performance_summary()
        
        self.test_results["api"] = {
            "total_requests": 100,
            "simulation_time": total_time,
            "performance_summary": api_summary
        }
        
        logger.info(f"   APIè¯·æ±‚æ¨¡æ‹Ÿæ—¶é—´: {total_time:.3f}s")
        logger.info(f"   å¹³å‡å“åº”æ—¶é—´: {api_summary.get('request_stats', {}).get('avg_response_time', 0)*1000:.1f}ms")
    
    async def test_async_performance(self):
        """æµ‹è¯•å¼‚æ­¥æ€§èƒ½"""
        logger.info("âš¡ æµ‹è¯•å¼‚æ­¥æ€§èƒ½...")
        
        # æ¨¡æ‹Ÿå¼‚æ­¥ä»»åŠ¡
        async def mock_async_task(task_id: int, delay: float):
            await asyncio.sleep(delay)
            return f"Task {task_id} completed"
        
        # å¹¶å‘æ‰§è¡Œæµ‹è¯•
        start_time = time.time()
        
        tasks = []
        for i in range(20):
            delay = 0.1 + (i % 3) * 0.05
            task = mock_async_task(i, delay)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        concurrent_time = time.time() - start_time
        
        # ä¸²è¡Œæ‰§è¡Œæµ‹è¯•
        start_time = time.time()
        
        for i in range(20):
            delay = 0.1 + (i % 3) * 0.05
            await mock_async_task(i, delay)
        
        sequential_time = time.time() - start_time
        
        # è®¡ç®—æ€§èƒ½æå‡
        performance_improvement = (sequential_time - concurrent_time) / sequential_time * 100
        
        self.test_results["async"] = {
            "concurrent_time": concurrent_time,
            "sequential_time": sequential_time,
            "performance_improvement_percent": performance_improvement,
            "tasks_completed": len(results)
        }
        
        logger.info(f"   å¹¶å‘æ‰§è¡Œæ—¶é—´: {concurrent_time:.3f}s")
        logger.info(f"   ä¸²è¡Œæ‰§è¡Œæ—¶é—´: {sequential_time:.3f}s")
        logger.info(f"   æ€§èƒ½æå‡: {performance_improvement:.1f}%")
    
    async def test_memory_simulation(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æ¨¡æ‹Ÿ"""
        logger.info("ğŸ§  æµ‹è¯•å†…å­˜ä½¿ç”¨æ¨¡æ‹Ÿ...")
        
        # æ¨¡æ‹Ÿå†…å­˜å¯†é›†å‹æ“ä½œ
        start_time = time.time()
        
        # åˆ›å»ºå¤§é‡å¯¹è±¡
        large_data = []
        for i in range(1000):
            data = {
                "id": i,
                "content": "x" * 1000,  # 1KBå­—ç¬¦ä¸²
                "metadata": {"timestamp": time.time(), "index": i}
            }
            large_data.append(data)
        
        creation_time = time.time() - start_time
        
        # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
        start_time = time.time()
        
        processed_data = []
        for item in large_data:
            processed = {
                "processed_id": item["id"] * 2,
                "processed_content": item["content"][:100],  # æˆªå–å‰100å­—ç¬¦
                "processed_at": time.time()
            }
            processed_data.append(processed)
        
        processing_time = time.time() - start_time
        
        # æ¸…ç†å†…å­˜
        start_time = time.time()
        del large_data
        del processed_data
        cleanup_time = time.time() - start_time
        
        self.test_results["memory"] = {
            "objects_created": 1000,
            "creation_time": creation_time,
            "processing_time": processing_time,
            "cleanup_time": cleanup_time,
            "total_time": creation_time + processing_time + cleanup_time
        }
        
        logger.info(f"   å¯¹è±¡åˆ›å»ºæ—¶é—´: {creation_time:.3f}s")
        logger.info(f"   æ•°æ®å¤„ç†æ—¶é—´: {processing_time:.3f}s")
        logger.info(f"   å†…å­˜æ¸…ç†æ—¶é—´: {cleanup_time:.3f}s")
    
    def calculate_performance_score(self) -> float:
        """è®¡ç®—ç»¼åˆæ€§èƒ½åˆ†æ•°"""
        score = 100.0
        
        # ç¼“å­˜æ€§èƒ½è¯„åˆ†ï¼ˆ25%ï¼‰
        cache_results = self.test_results.get("cache", {})
        cache_ops_per_sec = cache_results.get("operations_per_second", 0)
        if cache_ops_per_sec < 1000:  # æœŸæœ›æ¯ç§’1000æ¬¡æ“ä½œ
            score -= (1000 - cache_ops_per_sec) / 1000 * 25
        
        # APIæ€§èƒ½è¯„åˆ†ï¼ˆ30%ï¼‰
        api_results = self.test_results.get("api", {})
        api_summary = api_results.get("performance_summary", {})
        avg_response_time = api_summary.get("request_stats", {}).get("avg_response_time", 0)
        if avg_response_time > 0.5:  # æœŸæœ›500msä»¥ä¸‹
            score -= min(30, (avg_response_time - 0.5) / 0.5 * 30)
        
        # å¼‚æ­¥æ€§èƒ½è¯„åˆ†ï¼ˆ25%ï¼‰
        async_results = self.test_results.get("async", {})
        performance_improvement = async_results.get("performance_improvement_percent", 0)
        if performance_improvement < 80:  # æœŸæœ›80%ä»¥ä¸Šæå‡
            score -= (80 - performance_improvement) / 80 * 25
        
        # å†…å­˜æ€§èƒ½è¯„åˆ†ï¼ˆ20%ï¼‰
        memory_results = self.test_results.get("memory", {})
        total_memory_time = memory_results.get("total_time", 0)
        if total_memory_time > 1.0:  # æœŸæœ›1ç§’ä»¥å†…
            score -= min(20, (total_memory_time - 1.0) * 20)
        
        return max(0, min(100, score))
    
    def generate_report(self) -> dict:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_time = time.time() - self.start_time
        performance_score = self.calculate_performance_score()
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        cache_results = self.test_results.get("cache", {})
        if cache_results.get("hit_rate", 0) < 90:
            recommendations.append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œè€ƒè™‘ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")
        
        api_results = self.test_results.get("api", {})
        api_summary = api_results.get("performance_summary", {})
        if api_summary.get("request_stats", {}).get("avg_response_time", 0) > 0.3:
            recommendations.append("APIå“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–å¤„ç†é€»è¾‘")
        
        async_results = self.test_results.get("async", {})
        if async_results.get("performance_improvement_percent", 0) < 70:
            recommendations.append("å¼‚æ­¥æ€§èƒ½æå‡æœ‰é™ï¼Œæ£€æŸ¥å¹¶å‘æ§åˆ¶ç­–ç•¥")
        
        return {
            "test_timestamp": datetime.now().isoformat(),
            "total_test_time": total_time,
            "performance_score": performance_score,
            "test_results": self.test_results,
            "recommendations": recommendations,
            "summary": {
                "cache_ops_per_second": cache_results.get("operations_per_second", 0),
                "api_avg_response_time_ms": api_summary.get("request_stats", {}).get("avg_response_time", 0) * 1000,
                "async_improvement_percent": async_results.get("performance_improvement_percent", 0),
                "memory_processing_time": self.test_results.get("memory", {}).get("total_time", 0)
            }
        }

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç®€åŒ–ç‰ˆæ€§èƒ½ä¼˜åŒ–æµ‹è¯•...")
    print("="*60)
    
    try:
        # åˆ›å»ºæµ‹è¯•å®ä¾‹
        test = SimplePerformanceTest()
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        await test.test_cache_performance()
        await test.test_database_simulation()
        await test.test_api_simulation()
        await test.test_async_performance()
        await test.test_memory_simulation()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = test.generate_report()
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"simple_performance_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # è¾“å‡ºç»“æœ
        print("\n" + "="*60)
        print("ğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“Š ç»¼åˆæ€§èƒ½åˆ†æ•°: {report['performance_score']:.1f}/100")
        print(f"â±ï¸ æ€»æµ‹è¯•æ—¶é—´: {report['total_test_time']:.2f}ç§’")
        print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        
        print(f"\nğŸ“ˆ å…³é”®æŒ‡æ ‡:")
        summary = report['summary']
        print(f"   ğŸ’¾ ç¼“å­˜æ“ä½œ/ç§’: {summary['cache_ops_per_second']:.0f}")
        print(f"   ğŸŒ APIå¹³å‡å“åº”æ—¶é—´: {summary['api_avg_response_time_ms']:.1f}ms")
        print(f"   âš¡ å¼‚æ­¥æ€§èƒ½æå‡: {summary['async_improvement_percent']:.1f}%")
        print(f"   ğŸ§  å†…å­˜å¤„ç†æ—¶é—´: {summary['memory_processing_time']:.3f}s")
        
        if report['recommendations']:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(report['recommendations'], 1):
                print(f"   {i}. {rec}")
        
        print("\nâœ… æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        return 0
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"Performance test failed: {e}", exc_info=True)
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
